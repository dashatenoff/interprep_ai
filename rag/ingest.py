# rag/ingest.py
import json
from pathlib import Path
import chromadb
from chromadb.config import Settings
import hashlib

BASE_DIR = Path(__file__).resolve().parent.parent
KNOWLEDGE_DIR = BASE_DIR / "knowledge"
PERSIST_DIR = BASE_DIR / "chroma_db"
COLLECTION_NAME = "interprep_knowledge"


def load_all_knowledge():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Ç–∏–ø—ã –∑–Ω–∞–Ω–∏–π"""
    documents = []
    doc_count = 0

    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã –∏–∑ JSON
    questions_file = KNOWLEDGE_DIR / "interview_questions.json"
    if questions_file.exists():
        with open(questions_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            for q in data.get("questions", []):
                text = f"–í–æ–ø—Ä–æ—Å: {q['question']}\n–û—Ç–≤–µ—Ç: {q['answer']}\n–¢–µ–º–∞: {q['topic']} | –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {q['category']} | –°–ª–æ–∂–Ω–æ—Å—Ç—å: {q['difficulty']} | –£—Ä–æ–≤–µ–Ω—å: {q['level']}"

                documents.append({
                    "text": text,
                    "metadata": {
                        "type": "interview_question",
                        "topic": q["topic"],
                        "category": q["category"],
                        "difficulty": q["difficulty"],
                        "level": q["level"],
                        "company": q.get("company", "general"),
                        "agent": "interviewer"
                    }
                })
                doc_count += 1
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {doc_count} –≤–æ–ø—Ä–æ—Å–æ–≤")

    # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞
    examples_file = KNOWLEDGE_DIR / "code_examples.json"
    if examples_file.exists():
        with open(examples_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            for ex in data.get("examples", []):
                text = f"–ü—Ä–∏–º–µ—Ä: {ex['title']}\n–Ø–∑—ã–∫: {ex['language']}\n–•–æ—Ä–æ—à–∏–π –∫–æ–¥:\n{ex['good_code']}\n\n–ü–ª–æ—Ö–æ–π –∫–æ–¥:\n{ex['bad_code']}\n\n–û–±—ä—è—Å–Ω–µ–Ω–∏–µ: {ex['explanation']}"

                documents.append({
                    "text": text,
                    "metadata": {
                        "type": "code_example",
                        "language": ex["language"],
                        "category": ex["category"],
                        "level": ex["level"],
                        "agent": "reviewer"
                    }
                })
                doc_count += 1
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data.get('examples', []))} –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–æ–¥–∞")

    # 3. –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–ª–∞–Ω—ã –æ–±—É—á–µ–Ω–∏—è
    plans_file = KNOWLEDGE_DIR / "learning_plan.json"
    if plans_file.exists():
        with open(plans_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            for plan in data.get("plans", []):
                for week in plan.get("weeks", []):
                    text = f"–ü–ª–∞–Ω –æ–±—É—á–µ–Ω–∏—è: {week['focus']}\n–ù–µ–¥–µ–ª—è: {week['week']}\n–¢–µ–º—ã: {', '.join(week['topics'])}\n–ó–∞–¥–∞—á–∏: {', '.join(week['tasks'])}\n–†–µ—Å—É—Ä—Å—ã: {', '.join(week['resources'])}"

                    documents.append({
                        "text": text,
                        "metadata": {
                            "type": "learning_plan",
                            "level": plan["level"],
                            "track": plan["track"],
                            "week": week["week"],
                            "focus": week["focus"],
                            "agent": "planner"
                        }
                    })
                    doc_count += 1
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data.get('plans', []))} –ø–ª–∞–Ω–æ–≤ –æ–±—É—á–µ–Ω–∏—è")

    # 4. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
    for txt_file in KNOWLEDGE_DIR.glob("*.txt"):
        if txt_file.name != "interview_questions.json" and txt_file.name != "code_examples.json":
            try:
                with open(txt_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∞–±–∑–∞—Ü—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
                    paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]

                    for i, para in enumerate(paragraphs[:10]):  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 10 –∞–±–∑–∞—Ü–µ–≤
                        documents.append({
                            "text": para,
                            "metadata": {
                                "type": "text_knowledge",
                                "source": txt_file.name,
                                "paragraph": i,
                                "agent": "general"
                            }
                        })
                        doc_count += 1

                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª: {txt_file.name}")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {txt_file.name}: {e}")

    return documents


def create_knowledge_base():
    """–°–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
    print("üöÄ –°–æ–∑–¥–∞—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π InterPrep AI...")
    print("=" * 50)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
    documents = load_all_knowledge()

    if not documents:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π!")
        print(f"–ü–æ–ª–æ–∂–∏—Ç–µ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫—É: {KNOWLEDGE_DIR}")
        print("–ù—É–∂–Ω—ã: interview_questions.json, code_examples.json, learning_plan.json")
        return None

    print(f"üìö –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")

    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    PERSIST_DIR.mkdir(exist_ok=True)

    # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ ChromaDB
    client = chromadb.PersistentClient(
        path=str(PERSIST_DIR),
        settings=Settings(anonymized_telemetry=False)
    )

    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é –µ—Å–ª–∏ –µ—Å—Ç—å
    try:
        client.delete_collection(COLLECTION_NAME)
        print("‚ôªÔ∏è  –£–¥–∞–ª–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è")
    except:
        pass

    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é
    collection = client.create_collection(
        name=COLLECTION_NAME,
        metadata={
            "description": "InterPrep AI Knowledge Base",
            "version": "1.0",
            "documents_count": len(documents)
        }
    )

    print("üì• –î–æ–±–∞–≤–ª—è—é –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –±–∞–∑—É...")

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    batch_size = 100
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i + batch_size]

        texts = [doc["text"] for doc in batch]
        metadatas = [doc["metadata"] for doc in batch]
        ids = [f"doc_{hashlib.md5(doc['text'].encode()).hexdigest()[:12]}_{j}"
               for j, doc in enumerate(batch, i)]

        collection.add(
            documents=texts,
            metadatas=metadatas,
            ids=ids
        )

        print(f"  –î–æ–±–∞–≤–ª–µ–Ω–æ {min(i + batch_size, len(documents))}/{len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    print("=" * 50)
    print(f"‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
    print(f"üìä –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {collection.count()}")
    print(f"üìÅ –†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ: {PERSIST_DIR}")
    print(f"üè∑Ô∏è  –ö–æ–ª–ª–µ–∫—Ü–∏—è: {COLLECTION_NAME}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    print("\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º:")
    types_count = {}
    for doc in documents:
        doc_type = doc["metadata"].get("type", "unknown")
        types_count[doc_type] = types_count.get(doc_type, 0) + 1

    for doc_type, count in types_count.items():
        print(f"  {doc_type}: {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    return collection


def test_knowledge_base():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Å–æ–∑–¥–∞–Ω–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π...")

    try:
        client = chromadb.PersistentClient(path=str(PERSIST_DIR))
        collection = client.get_collection(COLLECTION_NAME)

        # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        test_queries = [
            ("Python –æ–ø–µ—Ä–∞—Ç–æ—Ä //", "interview_question"),
            ("SQL JOIN –æ—Ç–ª–∏—á–∏–µ", "interview_question"),
            ("–¥–µ–∫–æ—Ä–∞—Ç–æ—Ä –ø—Ä–∏–º–µ—Ä", "code_example"),
            ("–ø–ª–∞–Ω –æ–±—É—á–µ–Ω–∏—è", "learning_plan")
        ]

        for query, expected_type in test_queries:
            results = collection.query(
                query_texts=[query],
                n_results=1,
                where={"type": expected_type} if expected_type else None
            )

            if results["documents"]:
                print(f"‚úÖ '{query}' -> –Ω–∞–π–¥–µ–Ω–æ: {results['documents'][0][0][:80]}...")
            else:
                print(f"‚ö†Ô∏è  '{query}' -> –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")


if __name__ == "__main__":
    collection = create_knowledge_base()
    if collection:
        test_knowledge_base()